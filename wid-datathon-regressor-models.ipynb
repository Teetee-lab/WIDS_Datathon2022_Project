{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T22:17:07.540578Z","iopub.execute_input":"2022-04-29T22:17:07.541185Z","iopub.status.idle":"2022-04-29T22:17:07.551225Z","shell.execute_reply.started":"2022-04-29T22:17:07.541143Z","shell.execute_reply":"2022-04-29T22:17:07.550211Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"![Climate change](https://assets.nrdc.org/sites/default/files/styles/header_background--retina/public/personalaction-kidsclimatechange-2400x1600.jpg?itok=PeSlNGgX)","metadata":{}},{"cell_type":"markdown","source":"## Business Problem\n\nClimate change is a globally relevant, urgent, and multi-faceted issue heavily impacted by energy policy and infrastructure. Addressing climate change involves mitigation (i.e. mitigating greenhouse gas emissions) and adaptation (i.e. preparing for unavoidable consequences). Mitigation of GHG emissions requires changes to electricity systems, transportation, buildings, industry, and land use.\n\nAccording to a report issued by the International Energy Agency (IEA), the lifecycle of buildings from construction to demolition were responsible for 37% of global energy-related and process-related CO2 emissions in 2020. Yet it is possible to drastically reduce the energy consumption of buildings by a combination of easy-to-implement fixes and state-of-the-art strategies. For example, retrofitted buildings can reduce heating and cooling energy requirements by 50-90 percent. Many of these energy efficiency measures also result in overall cost savings and yield other benefits, such as cleaner air for occupants. This potential can be achieved while maintaining the services that buildings provide.","metadata":{}},{"cell_type":"code","source":"#import necessary liabries for data exploration and machine learning\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nimport itertools\nimport math\nimport time\nimport pickle\n\nimport statsmodels.api as sm\nimport category_encoders as ce\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold\nfrom scipy import stats\nfrom sklearn import svm\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,PolynomialFeatures \nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import LinearSVR\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import HuberRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:07.553121Z","iopub.execute_input":"2022-04-29T22:17:07.554198Z","iopub.status.idle":"2022-04-29T22:17:07.572389Z","shell.execute_reply.started":"2022-04-29T22:17:07.554155Z","shell.execute_reply":"2022-04-29T22:17:07.571551Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"I will be bringing in one of the functions I used for one of my projects, it is going to be useful throughout the training process to evaluate the model.","metadata":{}},{"cell_type":"code","source":"#Function to assist with evaluating the models\ndef evaluate (estimator, X_train,y_train, X_val, y_val):\n    \"\"\"\n    Evaluate the amount of error between the model's predictions \n    and actiual values for both train and test set\n    \n    Parameters:\n    y_train - array like, actual values for 'price' \n    train_preds - array like predicted values for 'price'\n    y_val/test -array like actual values for 'price'\n    val_preds/test - array like predicted values for 'price'\n    returns:\n    None\n    \"\"\"\n    \n   #predict the target on the training and validation data\n    train_preds = estimator.predict(X_train)\n    val_preds = estimator.predict(X_val)\n    \n    print(\"Scores\")\n    \n    print(f\"Train score: {train_preds}\") #training score\n    print(f\"validation score: {val_preds}\") #validation/Test score\n    \n    print(\"Rsquared:\")\n\n    print(f\"Train R2: {r2_score(y_train,train_preds):.4f}\") #rsquare metrics for train\n    print(f\"Validation R2: {r2_score(y_val,val_preds):.4f}\") #rsquare metrics for val/test\n\n    print(\"-----\")\n\n    print(\"Root Mean squared Error:\")\n\n    print(f\"Train RMSE: {mean_squared_error(y_train,train_preds,squared = False):.2f}\") #rmse metrics for train\n    print(f\"Validation RMSE: {mean_squared_error(y_val,val_preds, squared = False):.2f}\") #rmse metrics for val/test\n\n    print(\"-----\")\n\n    print(\" Mean absolute Error:\")\n\n    print(f\"Train MAE: {mean_absolute_error(y_train,train_preds):.2f}\") #MAE metrics for train\n    print(f\"Validation MAE: {mean_absolute_error(y_val,val_preds):.2f}\") #MAE metrics for val/test\n    \n    #calculate the residual\n    train_residual = y_train - train_preds #residual for train\n    val_residual = y_val - val_preds #residual for val / test\n    \n    #QQplot to check the normality of our \n    sm.qqplot(val_residual,line ='45',fit =True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:07.574639Z","iopub.execute_input":"2022-04-29T22:17:07.575391Z","iopub.status.idle":"2022-04-29T22:17:07.586417Z","shell.execute_reply.started":"2022-04-29T22:17:07.575350Z","shell.execute_reply":"2022-04-29T22:17:07.585529Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## Data Understanding\n\nThe WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\nYou are provided with two datasets: (1) the training dataset where the observed values of the Site EUI for each row is provided and (2) the test dataset where we withhold the observed values of the Site EUI for each row. To participate in the Datathon, you will submit a solution file containing the predicted Site EUI values for each row in the test dataset. The predicted values you submit will be compared against the observed Site EUI values for the test dataset and this will determine your standing on the Leaderboard during the competition as well as your final standing when the competition closes.\n\n\n**Features**\n\n- id: building id\n\n- Year_Factor: anonymized year in which the weather and energy usage factors were observed\n\n- State_Factor: anonymized state in which the building is located\n\n- building_class: building classification\n\n- facility_type: building usage type\n\n- floor_area: floor area (in square feet) of the building\n\n- year_built: year in which the building was constructed\n\n- energy_star_rating: the energy star rating of the building\n\n- ELEVATION: elevation of the building location\n\n- january_min_temp: minimum temperature in January (in Fahrenheit) at the location of the building\n\n- january_avg_temp: average temperature in January (in Fahrenheit) at the location of the building\n\n- january_max_temp: maximum temperature in January (in Fahrenheit) at the location of the building\n\n- cooling_degree_days: cooling degree day for a given day is the number of degrees where the daily average temperature exceeds 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n\n- heating_degree_days: heating degree day for a given day is the number of degrees where the daily average temperature falls under 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n\n- precipitation_inches: annual precipitation in inches at the location of the building\n\n- snowfall_inches: annual snowfall in inches at the location of the building\n\n- snowdepth_inches: annual snow depth in inches at the location of the building\n\n- avg_temp: average temperature over a year at the location of the building\n\n- days_below_30F: total number of days below 30 degrees Fahrenheit at the location of the building\n\n- days_below_20F: total number of days below 20 degrees Fahrenheit at the location of the building\n\n- days_below_10F: total number of days below 10 degrees Fahrenheit at the location of the building\n\n- days_below_0F: total number of days below 0 degrees Fahrenheit at the location of the building\n\n- days_above_80F: total number of days above 80 degrees Fahrenheit at the location of the building\n\n- days_above_90F: total number of days above 90 degrees Fahrenheit at the location of the building\n\n- days_above_100F: total number of days above 100 degrees Fahrenheit at the location of the building\n\n- days_above_110F: total number of days above 110 degrees Fahrenheit at the location of the building\n\n- direction_max_wind_speed: wind direction for maximum wind speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n\n- direction_peak_wind_speed: wind direction for peak wind gust speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n\n- max_wind_speed: maximum wind speed at the location of the building\n\n- days_with_fog: number of days with fog at the location of the building\n\n **Target**\n- site_eui: Site Energy Usage Intensity is the amount of heat and electricity consumed by a building as reflected in utility bills","metadata":{}},{"cell_type":"markdown","source":"Data has already been split so I will only be working with the train dataset and then vote for the best model to predict the test dataset.","metadata":{}},{"cell_type":"code","source":"#read the train data\ntrain_data = pd.read_csv(\"/kaggle/input/widsdatathon2022/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/widsdatathon2022/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:07.588539Z","iopub.execute_input":"2022-04-29T22:17:07.589290Z","iopub.status.idle":"2022-04-29T22:17:08.173880Z","shell.execute_reply.started":"2022-04-29T22:17:07.589239Z","shell.execute_reply":"2022-04-29T22:17:08.172551Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"#view the first ten rows of the data\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:08.180641Z","iopub.execute_input":"2022-04-29T22:17:08.183380Z","iopub.status.idle":"2022-04-29T22:17:08.241442Z","shell.execute_reply.started":"2022-04-29T22:17:08.183307Z","shell.execute_reply":"2022-04-29T22:17:08.240551Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#check the data info\ntrain_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:08.242663Z","iopub.execute_input":"2022-04-29T22:17:08.243868Z","iopub.status.idle":"2022-04-29T22:17:08.318938Z","shell.execute_reply.started":"2022-04-29T22:17:08.243793Z","shell.execute_reply":"2022-04-29T22:17:08.318112Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#let's view the shape of the data\ntrain_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:08.324005Z","iopub.execute_input":"2022-04-29T22:17:08.324841Z","iopub.status.idle":"2022-04-29T22:17:08.334453Z","shell.execute_reply.started":"2022-04-29T22:17:08.324794Z","shell.execute_reply":"2022-04-29T22:17:08.332282Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"#view the missing values per features in percentage\ntrain_data.isna().mean().sort_values().plot(\n    kind=\"bar\", figsize=(25, 10),\n    title=\"Percentage of missing values per feature\",\n    ylabel=\"Ratio of missing values per feature\");","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:08.336403Z","iopub.execute_input":"2022-04-29T22:17:08.336744Z","iopub.status.idle":"2022-04-29T22:17:11.073708Z","shell.execute_reply.started":"2022-04-29T22:17:08.336700Z","shell.execute_reply":"2022-04-29T22:17:11.072909Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"There are 55.2% missing values in direction_peak_wind_speed columns that are missing, 54.23% values in max_wind_speed and direction_max_wind_speed. Also 60.1% missing values from the days_with_fog columns. Those features have more than 50% missing values. So let's go ahead and drop any features with more than 15% missing values. ","metadata":{}},{"cell_type":"code","source":"#drop features with more than 15% missing values\ntrain_data = train_data.dropna(thresh = train_data.shape[0] *0.85, axis=1)\ntrain_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.075005Z","iopub.execute_input":"2022-04-29T22:17:11.075317Z","iopub.status.idle":"2022-04-29T22:17:11.123489Z","shell.execute_reply.started":"2022-04-29T22:17:11.075280Z","shell.execute_reply":"2022-04-29T22:17:11.122616Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#check if there is duplicated values\ntrain_data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.124921Z","iopub.execute_input":"2022-04-29T22:17:11.125262Z","iopub.status.idle":"2022-04-29T22:17:11.231548Z","shell.execute_reply.started":"2022-04-29T22:17:11.125226Z","shell.execute_reply":"2022-04-29T22:17:11.230737Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#check the remaining missing values.\ntrain_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.232926Z","iopub.execute_input":"2022-04-29T22:17:11.233289Z","iopub.status.idle":"2022-04-29T22:17:11.270393Z","shell.execute_reply.started":"2022-04-29T22:17:11.233251Z","shell.execute_reply":"2022-04-29T22:17:11.269454Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"This is great we can go ahead and fill the values since it is leass than 15%.","metadata":{}},{"cell_type":"code","source":"#fill all the nan values with zero\n#check if there is still null values in the dataset\ntrain_data = train_data.fillna(0, axis =1)\ntrain_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.271570Z","iopub.execute_input":"2022-04-29T22:17:11.272867Z","iopub.status.idle":"2022-04-29T22:17:11.343373Z","shell.execute_reply.started":"2022-04-29T22:17:11.272823Z","shell.execute_reply":"2022-04-29T22:17:11.342695Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"#view the statistical summary of the data\ntrain_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.346916Z","iopub.execute_input":"2022-04-29T22:17:11.347128Z","iopub.status.idle":"2022-04-29T22:17:11.564431Z","shell.execute_reply.started":"2022-04-29T22:17:11.347102Z","shell.execute_reply":"2022-04-29T22:17:11.563575Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#let's view the columns\ntrain_data.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.565832Z","iopub.execute_input":"2022-04-29T22:17:11.566110Z","iopub.status.idle":"2022-04-29T22:17:11.572248Z","shell.execute_reply.started":"2022-04-29T22:17:11.566060Z","shell.execute_reply":"2022-04-29T22:17:11.571272Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"#checking for columns with unique value\ntrain_data[[col for col in train_data.columns if train_data[col].dtype == 'object']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.573909Z","iopub.execute_input":"2022-04-29T22:17:11.574522Z","iopub.status.idle":"2022-04-29T22:17:11.647605Z","shell.execute_reply.started":"2022-04-29T22:17:11.574306Z","shell.execute_reply":"2022-04-29T22:17:11.646830Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"#view the target\ntrain_data['site_eui']","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.649181Z","iopub.execute_input":"2022-04-29T22:17:11.649482Z","iopub.status.idle":"2022-04-29T22:17:11.656113Z","shell.execute_reply.started":"2022-04-29T22:17:11.649449Z","shell.execute_reply":"2022-04-29T22:17:11.655240Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"train_data['site_eui'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.657766Z","iopub.execute_input":"2022-04-29T22:17:11.658412Z","iopub.status.idle":"2022-04-29T22:17:11.673372Z","shell.execute_reply.started":"2022-04-29T22:17:11.658365Z","shell.execute_reply":"2022-04-29T22:17:11.672638Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"#visualize the target\ntrain_data.hist(column='site_eui', bins = 50, figsize=(10,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:11.674873Z","iopub.execute_input":"2022-04-29T22:17:11.675328Z","iopub.status.idle":"2022-04-29T22:17:12.156766Z","shell.execute_reply.started":"2022-04-29T22:17:11.675292Z","shell.execute_reply":"2022-04-29T22:17:12.156058Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"As you can see that the target is positively skewed to the right showing that there are outliers. Let's go ahead and check the boundary values.","metadata":{"execution":{"iopub.status.busy":"2022-04-28T21:59:10.476309Z","iopub.execute_input":"2022-04-28T21:59:10.476676Z","iopub.status.idle":"2022-04-28T21:59:10.483865Z","shell.execute_reply.started":"2022-04-28T21:59:10.47664Z","shell.execute_reply":"2022-04-28T21:59:10.482423Z"}}},{"cell_type":"code","source":"#check the boundary values\nprint(\"Highest allowed\",train_data['site_eui'].mean() + 3 *train_data['site_eui'].std())\nprint(\"Lowest allowed\",train_data['site_eui'].mean() - 3 *train_data['site_eui'].std())","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.159881Z","iopub.execute_input":"2022-04-29T22:17:12.160079Z","iopub.status.idle":"2022-04-29T22:17:12.172399Z","shell.execute_reply.started":"2022-04-29T22:17:12.160054Z","shell.execute_reply":"2022-04-29T22:17:12.171545Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Now let's find the outliers","metadata":{}},{"cell_type":"code","source":"#check for outliers\noutliers = train_data[(train_data['site_eui'] > 257.35) | (train_data['site_eui'] > -92.18)]\noutliers","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.174793Z","iopub.execute_input":"2022-04-29T22:17:12.175346Z","iopub.status.idle":"2022-04-29T22:17:12.232224Z","shell.execute_reply.started":"2022-04-29T22:17:12.175309Z","shell.execute_reply":"2022-04-29T22:17:12.231381Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Now let's trim off the outliers, cap on it and then apply the capping.","metadata":{}},{"cell_type":"code","source":"#trimming off the outliers\ntrain_data =train_data[(train_data['site_eui'] < 257.35)&(train_data['site_eui'] > -92.18)]\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.233263Z","iopub.execute_input":"2022-04-29T22:17:12.233476Z","iopub.status.idle":"2022-04-29T22:17:12.295349Z","shell.execute_reply.started":"2022-04-29T22:17:12.233449Z","shell.execute_reply":"2022-04-29T22:17:12.294574Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"#Capping on outliers\nupper_limit = train_data['site_eui'].mean() + 3 *train_data['site_eui'].std()\nlower_limit = train_data['site_eui'].mean() - 3 *train_data['site_eui'].std()\n#Now let's apply the Capping\ntrain_data['site_eui'] = np.where(train_data['site_eui'] > upper_limit,\n                               upper_limit,np.where(train_data['site_eui'] < lower_limit,\n                                                    lower_limit,train_data['site_eui']))\n#reset index\ntrain_data =train_data.reset_index(drop=True)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.297928Z","iopub.execute_input":"2022-04-29T22:17:12.298435Z","iopub.status.idle":"2022-04-29T22:17:12.389388Z","shell.execute_reply.started":"2022-04-29T22:17:12.298389Z","shell.execute_reply":"2022-04-29T22:17:12.388421Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#view the summary statistics of the target\ntrain_data['site_eui'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.390945Z","iopub.execute_input":"2022-04-29T22:17:12.391406Z","iopub.status.idle":"2022-04-29T22:17:12.411837Z","shell.execute_reply.started":"2022-04-29T22:17:12.391362Z","shell.execute_reply":"2022-04-29T22:17:12.411003Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#Now let's plot the histogram again to see if the outlier has been removed.\ntrain_data.hist(column='site_eui', bins = 50, figsize=(10,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.413378Z","iopub.execute_input":"2022-04-29T22:17:12.413868Z","iopub.status.idle":"2022-04-29T22:17:12.853656Z","shell.execute_reply.started":"2022-04-29T22:17:12.413829Z","shell.execute_reply":"2022-04-29T22:17:12.852947Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"Now if you look at the histogram, you will see that the target has been normalize. All outliers has been removed which give us similar data across all the data. Let's go ahead and check if there ia ny multicollinearity among the features and target.","metadata":{}},{"cell_type":"code","source":"#let's visualize the multicolinearity among the features.\nsns.set(font_scale =2)\nplt.figure(figsize = (60,100))\nax = sns.heatmap(train_data.corr(),mask =np.triu(np.ones_like(train_data.corr(),dtype = bool)), \n            annot = True,cmap =sns.color_palette('rocket',7),\n            linewidths=2,linecolor='white',fmt='.2f',annot_kws={\"size\":15})\nplt.title(\"MultiCollinearity Btw target and Features \")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:12.855079Z","iopub.execute_input":"2022-04-29T22:17:12.855520Z","iopub.status.idle":"2022-04-29T22:17:25.773074Z","shell.execute_reply.started":"2022-04-29T22:17:12.855482Z","shell.execute_reply":"2022-04-29T22:17:25.770762Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"#Using the heatmap to view the feature that correlate with the target.\nplt.figure(figsize = (20,20))\nax =sns.heatmap(abs(train_data.corr())[['site_eui']], annot = True);\nplt.title(\"Correlation between the target and features\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:25.774248Z","iopub.execute_input":"2022-04-29T22:17:25.774932Z","iopub.status.idle":"2022-04-29T22:17:27.145213Z","shell.execute_reply.started":"2022-04-29T22:17:25.774897Z","shell.execute_reply":"2022-04-29T22:17:27.144522Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"Looking at the graph above you can see that there is a high multi-collinearity between the target and January minimum temperature,February average temp and the cooling degree days.Now let us go ahead and split our data into dependent and independent variables","metadata":{}},{"cell_type":"code","source":"#define X and y\nX = train_data.drop('site_eui',axis=1)\ny= train_data['site_eui']\n#view the shape of the data\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.146468Z","iopub.execute_input":"2022-04-29T22:17:27.147198Z","iopub.status.idle":"2022-04-29T22:17:27.167328Z","shell.execute_reply.started":"2022-04-29T22:17:27.147162Z","shell.execute_reply":"2022-04-29T22:17:27.166603Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"#view the shape of the data\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.168429Z","iopub.execute_input":"2022-04-29T22:17:27.168702Z","iopub.status.idle":"2022-04-29T22:17:27.176541Z","shell.execute_reply.started":"2022-04-29T22:17:27.168653Z","shell.execute_reply":"2022-04-29T22:17:27.175667Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"#View the first ten rows of the dependent features\nX.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.178247Z","iopub.execute_input":"2022-04-29T22:17:27.178569Z","iopub.status.idle":"2022-04-29T22:17:27.207977Z","shell.execute_reply.started":"2022-04-29T22:17:27.178533Z","shell.execute_reply":"2022-04-29T22:17:27.207128Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"#view the first ten rows of the target \ny.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.209558Z","iopub.execute_input":"2022-04-29T22:17:27.209888Z","iopub.status.idle":"2022-04-29T22:17:27.216980Z","shell.execute_reply.started":"2022-04-29T22:17:27.209845Z","shell.execute_reply":"2022-04-29T22:17:27.216105Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"#let's view the unique values and classify them into it's unique categories\nnum_cols = []\nohe_cols = []\nfreq_cols =[]\n\nfor col in X.columns:\n    if X[col].dtype in ['float64', 'int64']:\n        num_cols.append(col)\n    elif X[col].nunique() < 10:\n        ohe_cols.append(col)\n    else:\n        freq_cols.append(col)\n        \n#print the list of numerical columns,categorical columns and frequency columns\nprint(f\"list of numerical columns: {num_cols}\")\nprint(\"--\")\nprint(f\"list of categorical columns:{ohe_cols}\")\nprint(\"--\")\nprint(f\"list of Frequency columns:{freq_cols}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.223028Z","iopub.execute_input":"2022-04-29T22:17:27.223724Z","iopub.status.idle":"2022-04-29T22:17:27.252913Z","shell.execute_reply.started":"2022-04-29T22:17:27.223660Z","shell.execute_reply":"2022-04-29T22:17:27.252226Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"It seems here that we only have two categorical variables and one freq column. Now we can go ahead split the data,and scale it so that we can have equal data among the records.","metadata":{}},{"cell_type":"code","source":"#split the data into training and validation data. \n#we will be using the validation data to train the algorithms\n#then use the test data on the final model.\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# create another train data and validation data from the split\ntrain_df = pd.concat([X_train,y_train], axis= 1)\nvalidation_df = pd.concat([X_val,y_val], axis =1)\n\ntrain_df.shape , validation_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.254146Z","iopub.execute_input":"2022-04-29T22:17:27.254394Z","iopub.status.idle":"2022-04-29T22:17:27.319272Z","shell.execute_reply.started":"2022-04-29T22:17:27.254361Z","shell.execute_reply":"2022-04-29T22:17:27.318496Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# set up pipeline for preprocessing \n# for numeric columns, we need to scale it\n# for unique value <3 columns, we need to one hot encode it\n# for unique value >3 columns, we need to frequency encode it\nohe_transformer = Pipeline(steps=[\n    ('ohe_imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n    ('oh_encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\nfreq_transformer = Pipeline(steps=[\n    ('freq_encoder', ce.count.CountEncoder(normalize=True, min_group_size=.05)),\n    ('freq_imputer', SimpleImputer(strategy='constant', fill_value=0))\n])\n\nnum_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n    \n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('ohe', ohe_transformer, ohe_cols),\n        ('freq', freq_transformer, freq_cols),\n        ('scaler', num_transformer, num_cols)\n    ])\n\npreprocessor.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.320660Z","iopub.execute_input":"2022-04-29T22:17:27.321544Z","iopub.status.idle":"2022-04-29T22:17:27.573362Z","shell.execute_reply.started":"2022-04-29T22:17:27.321501Z","shell.execute_reply":"2022-04-29T22:17:27.572563Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"Now let's go ahead and build our baseline model so that we can use the scoring to train our data and compare across other algorithms.","metadata":{}},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"#We will be using LinearRegression as the baseline model\nbaseline = Pipeline(steps = [('preprocessor', preprocessor),\n                             ('reg', LinearRegression())])\nbaseline.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:27.574604Z","iopub.execute_input":"2022-04-29T22:17:27.574879Z","iopub.status.idle":"2022-04-29T22:17:28.084773Z","shell.execute_reply.started":"2022-04-29T22:17:27.574844Z","shell.execute_reply":"2022-04-29T22:17:28.083863Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"#Use the function to evaluate the baseline model.\n#evalutae the baseline model\nevaluate(baseline, X_train,y_train, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:28.086270Z","iopub.execute_input":"2022-04-29T22:17:28.086562Z","iopub.status.idle":"2022-04-29T22:17:28.611229Z","shell.execute_reply.started":"2022-04-29T22:17:28.086513Z","shell.execute_reply":"2022-04-29T22:17:28.610525Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"As you can see the baseline model is not doing good with a R-squared score of 12%. It simply mean that the model is over-fitting compared to the number of data sample with a very high root mean square error.That said, let's go ahead and build different model and then tune them with different parameters.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering with Gridsearch cv","metadata":{}},{"cell_type":"markdown","source":"- #### First Model with Gridsearch","metadata":{}},{"cell_type":"code","source":"#Initialize the DecisionTreeRegressor\nd_tree = Pipeline(steps = [('preprocessor', preprocessor),\n                             ('model', DecisionTreeRegressor())])\nparam = [{'model__criterion':['squared_error','friedman_mse'],#gridsearch using different parameters\n          'model__splitter':['best'],\n          'model__max_depth': [5,10,20],\n          'model__min_samples_split':[10,100],\n          'model__max_features': ['auto','sqrt']}]\n\ngrid = GridSearchCV(estimator= d_tree,\n                    param_grid = param, scoring= 'r2')\nresult = grid.fit(X_train,y_train) #fit the result on train data\nresult.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:17:28.612785Z","iopub.execute_input":"2022-04-29T22:17:28.613265Z","iopub.status.idle":"2022-04-29T22:18:22.827368Z","shell.execute_reply.started":"2022-04-29T22:17:28.613228Z","shell.execute_reply":"2022-04-29T22:18:22.826597Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"#view the best score from the parameter\nresult.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:18:22.828465Z","iopub.execute_input":"2022-04-29T22:18:22.828737Z","iopub.status.idle":"2022-04-29T22:18:22.834405Z","shell.execute_reply.started":"2022-04-29T22:18:22.828702Z","shell.execute_reply":"2022-04-29T22:18:22.833692Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"#evaluate the best estimator\nevaluate(result.best_estimator_,X_train,y_train, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:18:22.835982Z","iopub.execute_input":"2022-04-29T22:18:22.836468Z","iopub.status.idle":"2022-04-29T22:18:23.299352Z","shell.execute_reply.started":"2022-04-29T22:18:22.836433Z","shell.execute_reply":"2022-04-29T22:18:23.298683Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"The Rsquare is showing a variance of 15% on the training data and 15% on the validation data with a Validation RMSE OF 34.35. The model is overfitting on the variance compared to the data sample.","metadata":{}},{"cell_type":"markdown","source":"- #### Let's build another regressor model using gridsearch cv.","metadata":{}},{"cell_type":"code","source":"#Initialize the Gradient boost Regressor\ngbr = Pipeline(steps = [('preprocessor', preprocessor),\n                             ('gbr model',GradientBoostingRegressor())])\nparam = [{'gbr model__loss': ['squared_error', 'quantile'],\n          'gbr model__max_features':['sqrt','auto','log2']}]\n\ngrid = GridSearchCV(estimator= gbr,\n                    param_grid = param, scoring= 'r2')\nresult = grid.fit(X_train,y_train)\nresult.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:18:23.300840Z","iopub.execute_input":"2022-04-29T22:18:23.301296Z","iopub.status.idle":"2022-04-29T22:22:25.972223Z","shell.execute_reply.started":"2022-04-29T22:18:23.301258Z","shell.execute_reply":"2022-04-29T22:22:25.971522Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"result.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:22:25.973823Z","iopub.execute_input":"2022-04-29T22:22:25.974340Z","iopub.status.idle":"2022-04-29T22:22:25.983387Z","shell.execute_reply.started":"2022-04-29T22:22:25.974297Z","shell.execute_reply":"2022-04-29T22:22:25.981932Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"evaluate(result.best_estimator_,X_train,y_train, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:22:25.987331Z","iopub.execute_input":"2022-04-29T22:22:25.987848Z","iopub.status.idle":"2022-04-29T22:22:26.572836Z","shell.execute_reply.started":"2022-04-29T22:22:25.987810Z","shell.execute_reply":"2022-04-29T22:22:26.572192Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"- #### Use Final Model to predict the test data","metadata":{}},{"cell_type":"code","source":"#Initialize Random Forest Regressor\nrf = Pipeline(steps = [('preprocessor', preprocessor),\n                        ('rf model',RandomForestRegressor(criterion = 'squared_error',\n                                                          max_features='auto',min_samples_split=10,n_estimators= 100))])\nrf.fit(X_train,y_train)#fit the model on a train data","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:22:26.574382Z","iopub.execute_input":"2022-04-29T22:22:26.574664Z","iopub.status.idle":"2022-04-29T22:23:17.136539Z","shell.execute_reply.started":"2022-04-29T22:22:26.574629Z","shell.execute_reply":"2022-04-29T22:23:17.135722Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"#cross validate the model\ncross_validate(rf,X_train,y_train,return_train_score =True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:23:17.137955Z","iopub.execute_input":"2022-04-29T22:23:17.138401Z","iopub.status.idle":"2022-04-29T22:26:50.621300Z","shell.execute_reply.started":"2022-04-29T22:23:17.138356Z","shell.execute_reply":"2022-04-29T22:26:50.620542Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"#evaluate the final model\nevaluate(rf,X_train,y_train, X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:26:50.622587Z","iopub.execute_input":"2022-04-29T22:26:50.622862Z","iopub.status.idle":"2022-04-29T22:26:52.985442Z","shell.execute_reply.started":"2022-04-29T22:26:50.622826Z","shell.execute_reply":"2022-04-29T22:26:52.984662Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest Regressor was able to outperformed the other models with a R^2 of 70% on training data and 28% on validation data with a root mean square error of 31.83. I will go ahead and use it to predict the test data.","metadata":{}},{"cell_type":"code","source":"#predict the model using the test data\ntest_data = pd.read_csv(\"/kaggle/input/widsdatathon2022/test.csv\")#read the test data\ntest_data = test_data.fillna(test_data.mean()) #fill the nan value with the test data mean.\nprediction = rf.predict(test_data) #prediction\ndf = pd.DataFrame({'Predicted': prediction}).sort_values(by ='Predicted',ascending = False)#put the prediction in a dataframe","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:31:11.279629Z","iopub.execute_input":"2022-04-29T22:31:11.280322Z","iopub.status.idle":"2022-04-29T22:31:11.570804Z","shell.execute_reply.started":"2022-04-29T22:31:11.280286Z","shell.execute_reply":"2022-04-29T22:31:11.569964Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"#submission\nsubmission =pd.read_csv(\"/kaggle/input/widsdatathon2022/sample_solution.csv\")\nsubmission['site_eui']= prediction\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T22:31:16.073129Z","iopub.execute_input":"2022-04-29T22:31:16.073529Z","iopub.status.idle":"2022-04-29T22:31:16.117602Z","shell.execute_reply.started":"2022-04-29T22:31:16.073495Z","shell.execute_reply":"2022-04-29T22:31:16.116800Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}